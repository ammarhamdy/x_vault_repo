Training data poisoning is a type of indirect prompt injection in which the data the model is trained on is compromised. 

This can cause the LLM to return intentionally wrong or otherwise misleading information.

This vulnerability can arise for several reasons, including:
- The model has been trained on data that has not been obtained from trusted sources.
- The scope of the dataset the model has been trained on is too broad.